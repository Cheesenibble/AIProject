{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "fQMwRnY-DCih",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.647661Z",
     "start_time": "2025-01-18T19:18:47.637734Z"
    }
   },
   "source": [
    "#import library (tensorflow bc it says its good for image recognition meaning it can tell for differences in between images)\n",
    "#https://www.nvidia.com/en-au/glossary/tensorflow/#:~:text=TensorFlow%20can%20be%20used%20to,such%20as%20partial%20differential%20equations.\n",
    "\n",
    "#Before starting\n",
    "#get python interpreter\n",
    "#cd path\\to\\project\n",
    "#python -m pip install --upgrade pip\n",
    "#pip install tensorflow\n",
    "#set python directory to PATH https://www.youtube.com/watch?v=AMAE0S_NzxE&t=52s do it when not working\n",
    "#pip install pillow\n",
    "# pip install --upgrade tensorflow keras numpy pandas scikit-learn pillow\n",
    "\n",
    "\n",
    "import tensorflow as tf #you have to do: pip install tensorflow\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "# sys.modules['Image'] = Image\n",
    "\n",
    "#Keras (api we use) we can change if needed\n",
    "\n",
    "#keras.models is where we get the type of model we use (Sequential)\n",
    "#https://keras.io/api/models/\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#keras.layers is the type of layers\n",
    "#https://keras.io/api/layers/   and use this link for details abt dense,flatten, and dropout    https://datascience.stackexchange.com/questions/44124/when-to-use-dense-conv1-2d-dropout-flatten-and-all-the-other-layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "#below is a example model\n",
    "#from tensorflow.keras.applications import ResNet50 (CNN model) bc it says it is a visual recognicition model when i searched it up. https://keras.io/api/applications/ (gives a list of models keras uses)\n",
    "#https://wandb.ai/mostafaibrahim17/ml-articles/reports/The-Basics-of-ResNet50---Vmlldzo2NDkwNDE2#:~:text=ResNet50%20is%20a%20deep%20learning,model%20is%2050%20layers%20deep.\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "#allows to rescale images and enhances different features over consecutive trials for the ai model(for it to tell differences)\n",
    "#https://www.isahit.com/blog/what-is-the-purpose-of-image-preprocessing-in-deep-learning#:~:text=Even%20though%20geometric%20transformations%20of,features%20crucial%20for%20subsequent%20processing.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#chatgpt said to use this for some reason but I only see it allows us to stop our model during training.\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VY4jWXoMJ4Hv",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.662934Z",
     "start_time": "2025-01-18T19:18:47.656968Z"
    }
   },
   "source": "#from google.colab import files",
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9sLsfURQ0QO",
    "outputId": "450efa5e-8801-4b65-b187-f6a5543c6e71",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.693877Z",
     "start_time": "2025-01-18T19:18:47.679998Z"
    }
   },
   "source": "#uploaded = files.upload()",
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": [
    "#!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "#!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "#!apt-get update -qq 2>&1 > /dev/null\n",
    "#!apt-get -y install -qq google-drive-ocamlfuse fuse"
   ],
   "metadata": {
    "id": "GPDyI27F5I23",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.725764Z",
     "start_time": "2025-01-18T19:18:47.711811Z"
    }
   },
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "error",
     "timestamp": 1736645381436,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "pfFpDUABG22S",
    "outputId": "64719f90-a924-4c00-ae03-0cc3746f13f0",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.757126Z",
     "start_time": "2025-01-18T19:18:47.743537Z"
    }
   },
   "source": [
    "#self explainitory just replace path/to/____ with the file path\n",
    "\n",
    "#need images\n",
    "#also note that there should be two folders in the training images/test images (ai and human made images just name the folder depending on each one)\n",
    "trainImages = \"C:/Users/junse/Downloads/aiTrainingImages\" #change this\n",
    "\n",
    "#need images for this part\n",
    "#note that there should NOT be two folders for testing data\n",
    "testImages = \"C:/Users/junse/Downloads/testingFolderMaking/testFolder0\" #also change this"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "error",
     "timestamp": 1736645339146,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "LR-XxRInHAVr",
    "outputId": "9eed693a-41de-4e0b-cfcb-0dd4a91a53ca",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.788133Z",
     "start_time": "2025-01-18T19:18:47.775102Z"
    }
   },
   "source": [
    "#this is what allows us to change how the ai will see the image\n",
    "#link below gives list of possible values although I didn't use all of them ex:brightness, zoom, etc\n",
    "#https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844\n",
    "train_dataGenerator = ImageDataGenerator(\n",
    "\n",
    "    #https://github.com/Arsey/keras-transfer-learning-for-oxford102/issues/1 (this is the reason why we rescale to 1./255)\n",
    "    rescale=1./255,\n",
    "\n",
    "    #during training the image can roate up to 30 degrees (you can change this value if you want up to 360 i think)\n",
    "    rotation_range=30,\n",
    "\n",
    "    #self explainatory, allows more \"data images\" with a limited set because we can move the image in all 4 directions, look at link below for examples\n",
    "    #https://stackoverflow.com/questions/62484597/understanding-width-shift-range-and-height-shift-range-arguments-in-kerass\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "\n",
    "    #similar to width/height shift range but it distorts the image instead of moving it\n",
    "    #https://stackoverflow.com/questions/57301330/what-exactly-the-shear-do-in-imagedatagenerator-of-keras\n",
    "    shear_range=0.2,\n",
    "\n",
    "    #magnification range\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    #it can flip the image\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    #its the method used to fill the empty pixals(like for example we shift the image a bit to the right we have empty pixels on the left) although I don't get how it works\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#we don't need any otherthing besides rescaling bc its the testing data set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "error",
     "timestamp": 1736645309888,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "uNrq3Tm9Igpl",
    "outputId": "f0c8ce44-ac13-4e6f-e31d-2127d9fd54d7",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:47.851176Z",
     "start_time": "2025-01-18T19:18:47.806736Z"
    }
   },
   "source": [
    "#theres a error bc its unable to find the file btw (its saying something abt a naming error but I'm really hoping that adding images to our training data and testing data will fix this)\n",
    "train_generator = train_dataGenerator.flow_from_directory(\n",
    "\n",
    "    #its the path to the training images\n",
    "    trainImages,\n",
    "\n",
    "    #it has to be (224,224) because resnet50 uses 244,244\n",
    "    target_size=(224, 224),\n",
    "\n",
    "    #the number of images for each training run (this case its 25 but you can change depending on choice)\n",
    "    batch_size=25,\n",
    "\n",
    "    #we are classifiying based on true or false so we can use binary as our classifiction mode (0 for ai, 1 for human or you can swap it)\n",
    "    class_mode='binary')\n",
    "\n",
    "#btw .flow_from_directory has 3 requirements to work, 1: the root directory must have 2 folders(1 for training, 1 for testing), 2: the training folder should have a amount of sub directoies depending on what we are training on (our case its ai/human so two sub dierctories)\n",
    "#3: training folder should only contain the test images\n",
    "#https://studymachinelearning.com/keras-imagedatagenerator-with-flow_from_directory/#:~:text=The%20flow_from_directory()%20method%20takes,are%20using%20flow_from_directory()%20method.\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "\n",
    "    #path to test images\n",
    "    testImages,\n",
    "\n",
    "    #the rest is the same you get it (i just changed batch size to 50)\n",
    "    target_size=(224, 224),\n",
    "    batch_size=50,\n",
    "    class_mode='binary')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2652,
     "status": "ok",
     "timestamp": 1735773563011,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "YI2VSY8KIhLW",
    "outputId": "65b907c2-19a9-4711-f3c2-fcd44cc8f644",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:48.829277Z",
     "start_time": "2025-01-18T19:18:47.869346Z"
    }
   },
   "source": [
    "#imagenet is us using pre-trained weigths from ImageNet(which are created using other datasets)\n",
    "#IMPORTANT, note that I think google colab has accecss to the ImageNet database but I don't myself (im requesting access still)\n",
    "#include_top=flase means that we remove the top layer of the model aka it allows us to use our own inputs for the model as well as controlling the output while being able to use the already exisiting weights from ImageNet.\n",
    "#the input_shape is just (width, height, and rgb(3 means all 3 colors))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H0YJFibKIi1j",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:48.860089Z",
     "start_time": "2025-01-18T19:18:48.847378Z"
    }
   },
   "source": [
    "#it means that the ResNet50 model cant be modified when we train our own model(meaning the layers that resnet50 uses wont be changed)\n",
    "base_model.trainable = False"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WBjQGJLrIl2t",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:49.029604Z",
     "start_time": "2025-01-18T19:18:48.878295Z"
    }
   },
   "source": [
    "#BTW THIS IS WHERE OUR OWN MODEL IS MADE, each line is a new layer, ex: base_model, flatten, dense, and the others its why we rename sequential into 'model'\n",
    "\n",
    "#also to note the Sequential is the type of model we use\n",
    "#link for a bit of explainition on sequential model    https://www.tensorflow.org/guide/keras/sequential_model\n",
    "model = Sequential([\n",
    "\n",
    "    #the pretrained model\n",
    "    base_model,\n",
    "\n",
    "    #layers\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    #it classifiys the output into either 0 or 1 (final layer which is the classification one)\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AwbGZGbrInNW",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:49.060159Z",
     "start_time": "2025-01-18T19:18:49.047552Z"
    }
   },
   "source": [
    "#compile model, meaning we find the values we need for science fair\n",
    "#also we need to compile model in order for it to convert its data into things we can understand\n",
    "#chat gpt the things im getting lazy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jN_U_zHaIocf",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:18:49.091697Z",
     "start_time": "2025-01-18T19:18:49.078326Z"
    }
   },
   "source": [
    "#summary, early stopping stops the model if there is no improvement, monitor is self explainatory(it looks for only 'val_loss', patience is the number of epochs that have to be the same,\n",
    "# and restore best weights mean that before it stops it picks the best layer)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "error",
     "timestamp": 1735773589859,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "9lt88EtGIqKt",
    "outputId": "e8dd86fe-a437-43b2-f015-e823dc6d26fc",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:22:06.815443Z",
     "start_time": "2025-01-18T19:18:49.113173Z"
    }
   },
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "history = model.fit(\n",
    "\n",
    "    #error bc in previous code it is doesn't exist atm (please i need help with this line train_generator = train_dataGenerator.flow_from_directory(  its actually ruining the rest of the code)\n",
    "    train_generator,\n",
    "    #// is the divide operator\n",
    "    #and look here for epoch explaination https://www.geeksforgeeks.org/epoch-in-machine-learning/\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10, #repeat\n",
    "    verbose=1, #turn on verbose\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // test_generator.batch_size,\n",
    "    callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junse\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 9s/step - accuracy: 0.5653 - loss: 6.3331 - val_accuracy: 0.5000 - val_loss: 9.6228\n",
      "Epoch 2/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6s/step - accuracy: 0.5573 - loss: 7.6435 - val_accuracy: 0.5000 - val_loss: 7.2379\n",
      "Epoch 3/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 6s/step - accuracy: 0.4453 - loss: 8.6778 - val_accuracy: 0.5000 - val_loss: 6.1046\n",
      "Epoch 4/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6s/step - accuracy: 0.5600 - loss: 4.9972 - val_accuracy: 0.5000 - val_loss: 3.3551\n",
      "Epoch 5/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6s/step - accuracy: 0.5480 - loss: 4.6097 - val_accuracy: 0.5400 - val_loss: 2.4460\n",
      "Epoch 6/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6s/step - accuracy: 0.5320 - loss: 4.9773 - val_accuracy: 0.5000 - val_loss: 5.1397\n",
      "Epoch 7/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 6s/step - accuracy: 0.5147 - loss: 3.8961 - val_accuracy: 0.5200 - val_loss: 2.9209\n",
      "Epoch 8/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 6s/step - accuracy: 0.4880 - loss: 3.7602 - val_accuracy: 0.5100 - val_loss: 2.6529\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T19:22:06.862562Z",
     "start_time": "2025-01-18T19:22:06.854589Z"
    }
   },
   "cell_type": "code",
   "source": "#ImportError: Could not import PIL.Image. The use of `load_img` requires PIL.",
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZMNx1w9qIrqb",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:22:11.108610Z",
     "start_time": "2025-01-18T19:22:06.889472Z"
    }
   },
   "source": [
    "#it just saves the model\n",
    "model.save('ScienceFair.keras')"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "error",
     "timestamp": 1735773596330,
     "user": {
      "displayName": "Cheese",
      "userId": "14393660696254717761"
     },
     "user_tz": 300
    },
    "id": "kKOmtvp1ItAq",
    "outputId": "2a7d6f97-b1b9-4f7a-d38f-b06eb8568fae",
    "ExecuteTime": {
     "end_time": "2025-01-18T19:22:29.551729Z",
     "start_time": "2025-01-18T19:22:11.126038Z"
    }
   },
   "source": [
    "#none of this works rn bc of the same reasons(test_generator file is not made)\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 5s/step - accuracy: 0.5400 - loss: 2.4976\n",
      "Test accuracy: 0.5400000214576721\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T19:22:30.071345Z",
     "start_time": "2025-01-18T19:22:30.058034Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "toc_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
